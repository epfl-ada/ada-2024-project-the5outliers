{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_loader import read_all\n",
    "from src.utils.helpers import game_voyage_sorting , plot_sankey_voyage\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ , df_categories, _ , _ , df_unfinished, df_finished, _, _ = read_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical corelation between paths metrics and voyage status (finished/unfinished) SEPARATELY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def calculate_correlations_with_voyage(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate Pearson and Spearman correlations between 'voyage' and a specified column.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column_name (str): The column to calculate correlation with 'voyage'.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the correlation results.\n",
    "    \"\"\"\n",
    "    # Ensure 'voyage' is numeric (convert True/False to 1/0)\n",
    "    df['voyage'] = df['voyage'].astype(int)\n",
    "\n",
    "    # Calculate Pearson and Spearman correlations\n",
    "    pearson_corr, pearson_p = pearsonr(df['voyage'], df[column_name])\n",
    "    spearman_corr, spearman_p = spearmanr(df['voyage'], df[column_name])\n",
    "    # WARNING: comparing oranges with apples here. First separate into voyage, non-voyage, then compare the means with t-test\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{column_name}:\")\n",
    "    print(f\"  Pearson correlation: {pearson_corr:.4f}, p-value: {pearson_p:.4e}\")\n",
    "    print(f\"  Spearman correlation: {spearman_corr:.4f}, p-value: {spearman_p:.4e}\\n\")\n",
    "\n",
    "df_finished = game_voyage_sorting(df_finished, df_categories, True, n=3)\n",
    "df_unfinished = game_voyage_sorting(df_unfinished, df_categories, True, n=3)\n",
    "\n",
    "# Fill missing values in 'rating' with the mean\n",
    "df_finished['rating'] = df_finished['rating'].fillna(df_finished['rating'].mean())\n",
    "\n",
    "# Convert 'type' column to a binary column 'timeout'\n",
    "df_unfinished['timeout'] = (df_unfinished['type'] == 'timeout').astype(int)\n",
    "\n",
    "# Example use-case (replace `df_finished` and `df_unfinished` with actual data)\n",
    "print(\"Finished paths:\")\n",
    "for metric in ['durationInSec', 'rating', 'cosine_similarity', 'shortest_path', 'path_length', 'back_clicks', 'categories_similarity']:\n",
    "    calculate_correlations_with_voyage(df_finished, metric)\n",
    "\n",
    "print(\"Unfinished paths:\")\n",
    "for metric in ['durationInSec', 'timeout', 'cosine_similarity', 'shortest_path', 'path_length', 'back_clicks', 'categories_similarity']:\n",
    "    calculate_correlations_with_voyage(df_unfinished, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study with paths merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_merged = pd.concat([df_finished, df_unfinished])\n",
    "# Sort paths into voyage and non-voyage\n",
    "paths_merged = game_voyage_sorting(paths_merged, df_categories, True, n=3)\n",
    "# If the the type is NaN tell that this path is finished\n",
    "paths_merged['type'] = paths_merged['type'].fillna('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = paths_merged.copy()\n",
    "\n",
    "# One-hot encode the 'type' column\n",
    "df = pd.get_dummies(df, columns=['type'])\n",
    "\n",
    "# Handle missing values (e.g., replace NaN in 'rating' with the mean)\n",
    "df['rating'] = df['rating'].fillna(df['rating'].mean())\n",
    "# Drop columns that are not useful for the analysis\n",
    "columns_to_drop = ['hashedIpAddress','path','Category Path','start_maincategory','end_maincategory','target']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Focus on correlation with 'voyage'\n",
    "voyage_correlation = correlation_matrix['voyage'].sort_values(ascending=False)\n",
    "print(\"Correlation of features with 'voyage':\\n\", voyage_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features to cluster paths\n",
    "\n",
    "Not trivial which features choose to cluster the paths to see if their correlate to being a voyage or not...\n",
    "\n",
    "We migth clusters multiples groups of features or do it separatly (What to try??)\n",
    "\n",
    "Features interesting \n",
    "- For df_finished: \n",
    "```features = ['durationInSec','rating','cosine_similarity','shortest_path','path_length', 'back_clicks', 'categories_similarity']```\n",
    "- For df_unfinsihed:\n",
    "Same but + ```type``` and without ```rating```\n",
    "\n",
    "Maybe also merged all paths... -> the **silhouette_score** for 2 cluster change if paths are merged or not (logic both type paths differ in logic and length)\n",
    "- df_finished : 0.6\n",
    "- df_unfinished : 0.2 (better at 4 cluster (0.35)) **without type column**\n",
    "- df_unfinished : 0.4 (better at 4 cluster (0.45)) **with type column**\n",
    "- df_finished + df_unfinished : 0.4 **without type column** (one options is to create 3 dummies: finish,  timeout, restart)\n",
    "\n",
    "=> mmm beter do it separatly to avoid the confounding of cluster by intrisic differences btw f and u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDEA**\n",
    "Cluster paths based on subsets of features that represent specific aspects of behavior or data:\n",
    "\n",
    "- Group A: Path Characteristics\n",
    "Features: ``durationInSec, path_length, back_clicks.``  \n",
    "Focus: Captures navigation dynamics (e.g., efficiency, hesitation).\n",
    "\n",
    "- Group B: Content Similarity\n",
    "Features: ``cosine_similarity, categories_similarity.``   \n",
    "Focus: Captures how similar the target and the source of the path are similar.\n",
    "\n",
    "- Group C: Performance Metrics\n",
    "Features: ``rating, shortest_path.``  \n",
    "Focus: Measures subjective and objective path quality.\n",
    "\n",
    "For the moment done with all meaningfull and not bias (towards voyage) features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_score_plot(df_scaled, max_clusters):\n",
    "    scores = []\n",
    "    for k in tqdm(range(2, max_clusters), desc=\"Calculating silhouette scores\"):\n",
    "        kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "        labels = kmeans.fit_predict(df_scaled)\n",
    "        scores.append(silhouette_score(df_scaled, labels))\n",
    "\n",
    "    plt.plot(range(2, max_clusters), scores, marker='o')\n",
    "    plt.title('Silhouette Score vs Number of Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distribution of voyages\n",
    "voyage_counts = df_finished['voyage'].value_counts()\n",
    "plt.figure()\n",
    "plt.pie(\n",
    "    voyage_counts,\n",
    "    labels=voyage_counts.index.astype(str),\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    ")\n",
    "plt.title('Distribution of Voyages in Finished Paths')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['durationInSec','cosine_similarity','shortest_path','path_length', 'back_clicks', 'categories_similarity'] # 'rating'\n",
    "# rating we could use the mean value or do clusters with only paths with ratting?\n",
    "\n",
    "# Normalize the features for clustering\n",
    "scaler = StandardScaler()\n",
    "df_scaled_f = scaler.fit_transform(df_finished[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score_plot(df_scaled_f, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, n_init='auto', random_state=42)\n",
    "df_finished['cluster'] = kmeans.fit_predict(df_scaled_f)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(df_scaled_f)\n",
    "df_finished['pca1'], df_finished['pca2'] = reduced_data[:, 0], reduced_data[:, 1]\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='pca1', y='pca2', hue='cluster', palette='viridis', data=df_finished,style='voyage')\n",
    "plt.title('Clusters and Voyage Status')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Voyage / Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix-like DataFrame\n",
    "confusion_matrix = (\n",
    "    df_finished.groupby('voyage')['cluster']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Cluster vs Voyage Confusion Matrix\")\n",
    "plt.ylabel(\"Voyage (True/False)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage size of each cluster\n",
    "cluster_percentages = df_finished['cluster'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Percentage size of each cluster:\")\n",
    "for cluster, percentage in cluster_percentages.items():\n",
    "    print(f\"Cluster {cluster}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distribution of voyages\n",
    "voyage_counts = df_unfinished['voyage'].value_counts()\n",
    "plt.figure()\n",
    "plt.pie(\n",
    "    voyage_counts,\n",
    "    labels=voyage_counts.index.astype(str),\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    ")\n",
    "plt.title('Distribution of Voyages in Unfinished Paths')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['durationInSec','cosine_similarity','shortest_path','path_length', 'back_clicks', 'categories_similarity']\n",
    "# timout is a binary variable: 0 is for restart and 1 is for timeout\n",
    "\n",
    "# Normalize the features for clustering\n",
    "scaler = StandardScaler()\n",
    "df_scaled_u = scaler.fit_transform(df_unfinished[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score_plot(df_scaled_u, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, n_init='auto', random_state=42)\n",
    "df_unfinished['cluster'] = kmeans.fit_predict(df_scaled_u)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(df_scaled_u)\n",
    "df_unfinished['pca1'], df_unfinished['pca2'] = reduced_data[:, 0], reduced_data[:, 1]\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='pca1', y='pca2', hue='cluster', palette='viridis', data=df_unfinished,style='voyage')\n",
    "plt.title('Clusters and Voyage Status')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Voyage / Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix-like DataFrame\n",
    "confusion_matrix = (\n",
    "    df_unfinished.groupby('voyage')['cluster']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Cluster vs Voyage Confusion Matrix\")\n",
    "plt.ylabel(\"Voyage (True/False)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage size of each cluster\n",
    "cluster_percentages = df_unfinished['cluster'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Percentage size of each cluster:\")\n",
    "for cluster, percentage in cluster_percentages.items():\n",
    "    print(f\"Cluster {cluster}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering \n",
    "$O(n^{2})$ memory complexity ! for paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "# Method: 'ward' minimizes variance; try 'single', 'complete', 'average' for other linkage criteria\n",
    "linkage_matrix = linkage(df_scaled, method='ward')\n",
    "\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, leaf_rotation=90., leaf_font_size=10.)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Assign cluster labels based on a distance threshold or number of clusters\n",
    "# Example: Cutting the dendrogram to form 3 clusters\n",
    "cluster_labels = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
    "\n",
    "# Add cluster labels to your dataset (if applicable)\n",
    "# df['cluster'] = cluster_labels  # Uncomment and replace df with your DataFrame\n",
    "\n",
    "# Print example cluster assignments\n",
    "print(\"Cluster labels:\", cluster_labels)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a directed graph with transition counts\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Example graph data: edges with weights (transition counts)\n",
    "# New edges with weights focusing on making node C central\n",
    "edges = [\n",
    "    ('A', 'C', 25),  # High weight to C from A\n",
    "    ('B', 'C', 30),  # High weight to C from B\n",
    "    ('C', 'D', 20),  # C connects to D with significant weight\n",
    "    ('C', 'E', 15),  # C connects to E with significant weight\n",
    "    ('D', 'C', 10),  # Return edge to C from D\n",
    "    ('E', 'C', 12),  # Return edge to C from E\n",
    "    ('F', 'C', 18),  # F strongly connects to C\n",
    "    ('C', 'F', 10)   # C connects back to F\n",
    "]\n",
    "\n",
    "\n",
    "# Add edges to the graph\n",
    "for u, v, weight in edges:\n",
    "    G.add_edge(u, v, weight=weight)\n",
    "\n",
    "# Step 2: Calculate centrality metrics\n",
    "pagerank = nx.pagerank(G, weight='weight')  # PageRank\n",
    "in_degree_centrality = nx.in_degree_centrality(G)  # In-degree centrality\n",
    "\n",
    "# Step 3: Identify frequent chains (high-weight paths)\n",
    "# Sort edges by weight\n",
    "frequent_chains = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G, seed=42)  # Layout for visualization\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10)\n",
    "labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.title(\"Directed Graph with Transition Counts\")\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"PageRank (centrality measure):\")\n",
    "for node, rank in pagerank.items():\n",
    "    print(f\"{node}: {rank:.4f}\")\n",
    "\n",
    "print(\"\\nIn-degree centrality:\")\n",
    "for node, centrality in in_degree_centrality.items():\n",
    "    print(f\"{node}: {centrality:.4f}\")\n",
    "\n",
    "print(\"\\nFrequent chains (high-weight edges):\")\n",
    "for u, v, data in frequent_chains:\n",
    "    print(f\"{u} -> {v} (weight: {data['weight']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
