{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style= \"color: #c7c9cf\">The5Outliers - </span><span style=\"background: linear-gradient(to right, #3458d6, #34d634); -webkit-background-clip: text; color: transparent;\">Wikispeedia Voyages</span><span style= \"color: #c7c9cf\">: why so many players pass through Geography or Countries to reach their target</span></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.stats import kurtosis\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from src.data_loader import *\n",
    "from src.helpers import *\n",
    "from src.models.networks import *\n",
    "from src.htmlParser import htmlParser\n",
    "\n",
    "parser = htmlParser()\n",
    "parser.load_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaning and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part loads, cleans the data, and defines useful datasets for further analysis. \n",
    "\n",
    "- `read_articles()` loads a list of valid article names, that is without special characters and discardig any invalid articles, like non-wikispedia articles or with missing categories\n",
    "- `parser.get_df_html_stats()` gathers wikispedia page statistics like number of links, link density, and structural information\n",
    "- `read_categories()` sorts for each category of a same article its different levels of sub categories \n",
    "- `read_links()` gathers all the links outward of a page \n",
    "- `read_shortest_path_matrix()` reads the matrix of shortest paths possible between two articles \n",
    "- `read_unfinished_paths()` and `read_finished_paths()` load the original unfinished and finished paths and clean them\n",
    "- `read_similartiy_matrix()` reads the matrix of semantic similarity between article names \n",
    "- `read_categories_matrix()` reads the matrix that describes the similarity between article's category levels and sub-levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article_names = read_articles() \n",
    "df_html_stats = parser.get_df_html_stats()\n",
    "df_categories = read_categories()\n",
    "df_links = read_links()\n",
    "df_shortest_path = read_shortest_path_matrix()\n",
    "df_unfinished = read_unfinished_paths()\n",
    "df_finished = read_finished_paths() \n",
    "df_sm = read_similartiy_matrix() \n",
    "df_scat = read_categories_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Add features to articles and paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Article features \n",
    "In and out degree of articles are added. \n",
    "- **In degree** of an article corresponds to the number of links on other pages targetting an article \n",
    "- **Out degree** of an article corresponds to the number of links towards other pages on this article\n",
    "\n",
    "Some notes:\n",
    "- Some articles have no articles leading to it \n",
    "- Some articles lead to no other articles\n",
    "- Links do not consider duplicates inside the page (each link is considered to appear once, but this will be relaxed with HTML parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = pd.DataFrame(df_article_names).copy()\n",
    "\n",
    "# Compute in-degree (number of times each article is a target link)\n",
    "in_degree = df_links.groupby('linkTarget').size().reset_index(name=\"in_degree\")\n",
    "# Compute out-degree (link density: number of times each article is a source link)\n",
    "out_degree = df_links.groupby('linkSource').size().reset_index(name=\"out_degree\")\n",
    "\n",
    "# Merge in-degree and out-degree with df_article_names\n",
    "df_article = df_article.merge(in_degree, left_on='article', right_on='linkTarget', how='left')\n",
    "df_article = df_article.merge(out_degree, left_on='article', right_on='linkSource', how='left')\n",
    "df_article = df_article.drop(columns=['linkTarget', 'linkSource'])\n",
    "\n",
    "# Fill NaN values with 0, assuming no links imply zero counts for those articles\n",
    "df_article = df_article.fillna(0).astype({'in_degree': 'int', 'out_degree': 'int'})\n",
    "\n",
    "# add the html stats to the articles\n",
    "df_html_stats = df_html_stats.rename(columns={'article_name': 'article'})\n",
    "df_article = pd.merge(df_article, df_html_stats, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Path features \n",
    "New metrics are added to characterize the game paths. Specifically:\n",
    "\n",
    "- **Path Length** which accounts for the total number of articles in a path\n",
    "\n",
    "- **Back Clicks** which indicates how many times the user revisited previous articles\n",
    "\n",
    "- **Cosine Similarity** which measures the semantic similarity between the source and target articles\n",
    "\n",
    "- **Shortest Path** which gives the length of the shortest path possible between the source and target articles\n",
    "\n",
    "- **Categories Similarity** which  measure the category similarity between the categories of the source and target articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add some useful metrics to each paths dataframe: shortest path, semantic similarity\n",
    "df_unfinished['cosine_similarity'] = df_unfinished.apply(lambda x: find_shortest_distance(x, df_sm), axis=1)\n",
    "df_unfinished['shortest_path'] = df_unfinished.apply(lambda x: find_shortest_distance(x, df_shortest_path), axis=1)\n",
    "df_unfinished['path_length'] = df_unfinished['path'].apply(lambda x: x.count(';') + 1)\n",
    "df_unfinished['back_clicks'] = df_unfinished['path'].apply(lambda x: x.count('<'))\n",
    "df_unfinished['categories_similarity'] = df_unfinished.apply(lambda x: find_shortest_distance(x, df_scat), axis=1)\n",
    "\n",
    "df_finished['cosine_similarity'] = df_finished.apply(lambda x: find_shortest_distance(x, df_sm), axis=1)\n",
    "df_finished['shortest_path'] = df_finished.apply(lambda x: find_shortest_distance(x, df_shortest_path), axis=1)\n",
    "df_finished['path_length'] = df_finished['path'].apply(lambda x: x.count(';') + 1)\n",
    "df_finished['back_clicks'] = df_finished['path'].apply(lambda x: x.count('<'))\n",
    "df_finished['categories_similarity'] = df_finished.apply(lambda x: find_shortest_distance(x, df_scat), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Core results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Article features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a. Distributions of words, links, and categories in articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = dict(zip(df_categories[\"article\"], df_categories[\"level_1\"]))\n",
    "df_html_stats[\"category\"] = df_html_stats[\"article_name\"].map(category_map)\n",
    "\n",
    "metrics = [\n",
    "    (\"total_words\", \"Number of Words in Article\"),\n",
    "    (\"abstract_words\", \"Number of Words in Article Abstract\"),\n",
    "    (\"link_density\", \"Links Density in Article\"),\n",
    "    (\"abstract_link_density\", \"Links Density in Article Abstract\"),\n",
    "    (\"num_sections\", \"Number of Sections in Article\"),\n",
    "    (\"num_subsections\", \"Number of Sub- Sections in Article\")\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
    "\n",
    "# Loop through metrics to create histograms\n",
    "for i, (metric, title) in enumerate(metrics):\n",
    "    row, col = divmod(i, 3)\n",
    "    sn.histplot(df_html_stats, x=metric, bins=30, kde=True, ax=ax[row, col])\n",
    "    ax[row, col].set_title(title)\n",
    "    if col == 1 or col == 2:\n",
    "        ax[row, col].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of Article Metrics\", y=1.05, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : The data is right-skewed for most of metrics, indicating that while most articles adhere to certain standards of complexity and length, a small subset stands out as particularly detailed or interconnected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Articles Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which categories feature the most complex articles? To answer this question, letâ€™s identify the most complex articles and the categories they belong to. We define complexity based on factors such as the number of words, links, and sections within each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define palette for categories\n",
    "df_article[\"category\"]=df_article[\"article\"].apply(lambda x: df_categories[df_categories[\"article\"]==x][\"level_1\"].values[0] if len(df_categories[df_categories[\"article\"]==x][\"category\"].values)>0 else \"None\")\n",
    "categories = sorted(df_article[\"category\"].unique())\n",
    "palette_category = sn.color_palette(\"tab20\", len(categories))\n",
    "color_mapping = dict(zip(categories, palette_category))\n",
    "\n",
    "def add_legend_category(fig, palette_category=palette_category, categories=categories, bbox_to_anchor=(1.15, 0.85)):\n",
    "\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color=color, linestyle='', markersize=10) \n",
    "            for color in palette_category]\n",
    "    labels = categories\n",
    "    fig.legend(\n",
    "        handles, \n",
    "        labels, \n",
    "        bbox_to_anchor=bbox_to_anchor, \n",
    "        title=\"Categories\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
    "\n",
    "# Define the parameters for each subplot\n",
    "metrics = [\n",
    "    \"total_words\", \"link_density\", \"num_sections\",\n",
    "    \"abstract_words\", \"abstract_link_density\", \"num_subsections\"\n",
    "]\n",
    "\n",
    "# Loop through metrics and plot\n",
    "for i, metric in enumerate(metrics):\n",
    "    row, col = divmod(i, 3)\n",
    "    order = df_html_stats.groupby(\"category\")[metric].mean().sort_values(ascending=False).reset_index()[\"category\"]\n",
    "    sn.barplot(\n",
    "        x=\"category\", \n",
    "        y=metric, \n",
    "        hue=\"category\", \n",
    "        palette=color_mapping, \n",
    "        data=df_html_stats, \n",
    "        ax=ax[row, col], \n",
    "        order=order\n",
    "    )\n",
    "    ax[row, col].legend_.remove() \n",
    "    ax[row, col].set_title(f'{metric.replace(\"_\", \" \").capitalize()} by Category')\n",
    "    ax[row, col].set_xticklabels([])\n",
    "    if row == 0 :\n",
    "        ax[row, col].set_xlabel('')\n",
    "\n",
    "add_legend_category(fig)\n",
    "\n",
    "plt.suptitle(\"Articles Complexity by Categories\", y=1, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : The category \"Countries\" stands out as a particularly complex topic, characterized by articles with the highest link density, the greatest number of category levels, and the longest abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Articles Popularity and Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a correlation between the number of links in an article and the frequency with which people interact with it in the game? Does this depend more on the in-degree or out-degree of the links? And is there a significant difference between the in-degree and out-degree of links? We will compare and investigate these factors to understand their impact on article interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "#Plot the most visited articles in finished paths\n",
    "all_articles = []\n",
    "df_finished['path'].apply(lambda x: all_articles.extend(x.split(';')))\n",
    "df_path_articles = pd.Series(all_articles).value_counts().rename_axis('article_name').reset_index(name='value_counts')\n",
    "df_path_articles[\"category\"]=df_path_articles[\"article_name\"].apply(lambda x: df_categories[df_categories[\"article\"]==x][\"level_1\"].values[0] if len(df_categories[df_categories[\"article\"]==x][\"category\"].values)>0 else \"None\")\n",
    "df_path_articles = df_path_articles[df_path_articles['article_name'] != '<']\n",
    "\n",
    "sn.barplot(x='value_counts', y='article_name', hue=\"category\", palette=color_mapping, data=df_path_articles.head(15), ax=ax[0])\n",
    "ax[0].set_title('Most visited articles in paths')\n",
    "ax[0].legend_.remove() \n",
    "\n",
    "for i, metric in enumerate([\"in_degree\", \"out_degree\"]):\n",
    "    sn.barplot(x=metric, y='article', hue=\"category\", palette=color_mapping, data=df_article.sort_values(metric, ascending=False).head(15), ax=ax[i+1])\n",
    "    ax[i+1].set_title(f'Articles with the most links ({metric.replace(\"_\", \" \").capitalize()}) (without duplicates)')\n",
    "    ax[i+1].legend_.remove()\n",
    "    ax[i+1].set_ylabel('')\n",
    "\n",
    "add_legend_category(fig)\n",
    "plt.suptitle(\"Correlation between article popularity and link density\", y=1, fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : We observe a significant overlap between the most visited articles and those with the highest degree. Additionally, the \"Countries\" category is prominently represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Analyse user behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for patterns in user behaviour and try to understand how we could measure whether a game was difficult or not. Many different metrics can be considered, for instance:\n",
    "- Game duration\n",
    "- Game path length\n",
    "- Difficulty rating given for finished paths\n",
    "- Number of back-clicks needed\n",
    "- Whether a game was finished or not\n",
    "- For unfinished games, how the game was abandoned\n",
    "\n",
    "A combination of these parameters can help finding in which games users struggled. This will then allow to assess whether players struggle less in <span style=\"background: linear-gradient(to right, #3458d6, #34d634); -webkit-background-clip: text; color: transparent;\">Wikispeedia Voyages</span> than in other paths.\n",
    "\n",
    "Let's first have a look of how the difficulty measures are distributed amongst each other for finished paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Path duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find medians and kurtosis for both finished and unfinished\n",
    "median_finished = df_finished['durationInSec'].median()\n",
    "kurtosis_finished = kurtosis(df_finished['durationInSec'])\n",
    "median_unfinished = df_unfinished.groupby('type')['durationInSec'].median()\n",
    "kurtosis_unfinished = df_unfinished.groupby('type')['durationInSec'].apply(kurtosis)\n",
    "\n",
    "print(f\"The median duration of finished paths is {median_finished:.0f} seconds\")\n",
    "print(f\"The median duration of all unfinished paths is {df_unfinished['durationInSec'].median():.0f} seconds, among which :\")\n",
    "print(f\"  - The median duration of restart paths is {median_unfinished['restart']:.0f} seconds\")\n",
    "print(f\"  - The median duration of timeout paths is {median_unfinished['timeout']:.0f} seconds\")\n",
    "\n",
    "print(f\"Kurtosis of finished game durations: {kurtosis_finished:.2f}\")\n",
    "print(f\"Kurtosis of restart game durations: {kurtosis_unfinished['restart']:.2f}\")\n",
    "print(f\"Kurtosis of timeout game durations: {kurtosis_unfinished['timeout']:.2f}\")\n",
    "\n",
    "# Plotting histograms\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Unfinished games\n",
    "ax1 = plt.subplot(121)\n",
    "sn.histplot(df_unfinished, x='durationInSec', hue='type', bins=100)\n",
    "plt.yscale('log')\n",
    "plt.title('Histogram of game duration for unfinished games')\n",
    "\n",
    "for t, median in median_unfinished.items():\n",
    "    plt.axvline(median, linestyle='--', color='black', linewidth=1, label=f'{t.capitalize()} Median: {median:.2f}')\n",
    "\n",
    "# Finished games\n",
    "plt.subplot(122, sharey=ax1)\n",
    "sn.histplot(df_finished, x='durationInSec', bins=100, alpha=0.5, color='y')\n",
    "plt.axvline(median_finished, linestyle='--', color='black', linewidth=1)\n",
    "plt.yscale('log')\n",
    "plt.title('Histogram of game duration for finished games')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timout paths are games where the player stopped playing for more that 30 minutes. If we set those aside, and look at the duration of paths where the players decided to restart a new game, the mediam duration of games is 114 seconds, approximately the same as finished path duration of 107 seconds.\n",
    "\n",
    "The kurtosis in finished games is much higher than in unfinished restart games (3973 vs 45), meaning that finished games are more heavy tailed. We can in fact see very little players restart after 3000 seconds, whereas many finished games go beyond 6000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Path duration and path length vs. user rating\n",
    "\n",
    "Let's first look at path duration distribution over different user rating. We found similar results for finished and unfinished paths, so let's have a look at finished one for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finished_strNaN = df_finished.copy()\n",
    "df_finished_strNaN['rating'] = df_finished_strNaN['rating'].fillna('NaN')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "metric = 'durationInSec' # change metric to 'path_length' for path length \n",
    "\n",
    "means, stds = [], []\n",
    "ax1 = plt.subplot(231)\n",
    "sn.histplot(df_finished[df_finished_strNaN['rating']=='NaN'], x=metric, bins=50, log_scale=True) \n",
    "plt.axvline(df_finished[df_finished_strNaN['rating']=='NaN'][metric].mean(), color='red')\n",
    "plt.yscale('log')\n",
    "plt.title('NaN')\n",
    "\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(231+i, sharex = ax1, sharey=ax1)\n",
    "    sn.histplot(df_finished[df_finished_strNaN['rating']==i], x=metric, bins=50, log_scale=True)\n",
    "    plt.axvline(df_finished[df_finished_strNaN['rating']==i][metric].mean(), color='red')\n",
    "    plt.yscale('log')\n",
    "    plt.title(i)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Path duration by rating', y=1.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of the path duration by rating are nice log-normal distributions! This means we can use the mean (location) and standard deviation (scale) to characterise them. Indeed, except for NaN, there is a steady increase of the path duration mean when rating goes up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at whether path duration is correlated with path length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path_stats_duration = pd.DataFrame()\n",
    "df_path_stats_duration['mean'] = df_finished.groupby('rating', dropna=False)['durationInSec'].mean()\n",
    "df_path_stats_duration['std'] = df_finished.groupby('rating', dropna=False)['durationInSec'].std()\n",
    "df_path_stats_duration['sem'] = df_finished.groupby('rating', dropna=False)['durationInSec'].sem()\n",
    "\n",
    "df_path_stats_length = pd.DataFrame()\n",
    "df_path_stats_length['mean'] = df_finished.groupby('rating', dropna=False)['path_length'].mean()\n",
    "df_path_stats_length['std'] = df_finished.groupby('rating', dropna=False)['path_length'].std()\n",
    "df_path_stats_length['sem'] = df_finished.groupby('rating', dropna=False)['path_length'].sem()\n",
    "\n",
    "df_path_stats = pd.concat([df_path_stats_duration, df_path_stats_length], axis=1, keys=['duration', 'length'])\n",
    "\n",
    "df_path_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['NaN', '1.0', '2.0', '3.0', '4.0', '5.0']\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "sn.barplot(df_finished_strNaN, x='rating', y='durationInSec', order=order, errorbar=('ci', 95))\n",
    "plt.subplot(122)\n",
    "sn.barplot(df_finished_strNaN, x='rating', y='path_length', order=order, errorbar=('ci', 95))\n",
    "plt.title('Duration and Path Length by Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear correlation between how long players took or how many clicks they made with the experienced difficulty rating. This means we can easily combine the two into a difficulty measure as they agree with each other on what players considered difficult. \n",
    "\n",
    "Let's now loow at back-clicks: could it indicate whether players had a hard time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.c Back-clicks\n",
    "The number of back-clicks made in a game may indicate players getting stuck.\n",
    "Lets investigate how this metric is related with player ratings, and if they are more prominent in certain categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will explain later (3.3) what filtering the most specific category really means\n",
    "df_categories_filtered = filter_most_specific_category(df_categories)\n",
    "paths_finished = extract_category_path(df_finished, df_categories_filtered)\n",
    "paths_finished = backtrack(paths_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "back_per_rating = paths_finished.groupby(\"rating\").agg({\"back_nb\": \"mean\", \"size\": \"mean\"}).reset_index()\n",
    "back_per_rating['Mean Back Clicks number'] = back_per_rating[\"back_nb\"]/back_per_rating[\"size\"]\n",
    "sn.barplot(x=\"rating\", y='Mean Back Clicks number', hue=\"rating\", data=back_per_rating, palette=sn.color_palette('viridis'), ax=ax[0])\n",
    "\n",
    "df_exploded = paths_finished.explode('category')\n",
    "category_back_mean = df_exploded.groupby(['category', 'rating']).size().reset_index(name='size')\n",
    "back_mean = df_exploded.groupby('category')[\"have_back\"].mean().reset_index().sort_values(by='have_back', ascending=False)\n",
    "category_back_mean = category_back_mean.merge(back_mean, on='category').sort_values(by='have_back', ascending=False)\n",
    "category_back_mean['rating_proportion'] = category_back_mean.groupby('category')['size'].transform(lambda x: x / x.sum())\n",
    "\n",
    "order = category_back_mean[\"category\"].unique()\n",
    "base_heights = category_back_mean[['category', 'have_back']].drop_duplicates().set_index('category')['have_back']\n",
    "df_pivot = category_back_mean.pivot(index='category', columns='rating', values='rating_proportion').fillna(0)\n",
    "df_pivot = df_pivot.reindex(order)\n",
    "\n",
    "colors = sn.color_palette('viridis')\n",
    "\n",
    "bottom = pd.Series([0] * len(df_pivot), index=df_pivot.index)\n",
    "for i, rating in enumerate(df_pivot.columns):\n",
    "    ax[1].bar(df_pivot.index, \n",
    "           height=df_pivot[rating] * base_heights,  \n",
    "           bottom=bottom * base_heights,           \n",
    "           label=f'Rating {rating}', \n",
    "           color=colors[i])\n",
    "    bottom += df_pivot[rating]\n",
    "\n",
    "ax[1].set_title('Mean Number of Path with Back Clicks by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Mean Number of Path with Back Clicks')\n",
    "plt.xticks(rotation=90)\n",
    "ax[0].set_title(\"Distribution of Back Clicks per Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backclicks are correlated with the difficulty rating, which is expected as the more difficult a game is, the more likely players are to get stuck and go back.\n",
    "The category with the most backclicks is \"Art\", and the one with the least is \"Countries\". This is interesting as \"Countries\" is also one of the most visited category, which might indicate that players are more familiar with it and therefore less likely to get stuck.\n",
    "Additionaly, The distribution of rating inside each category seams quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Look at articles as a categories types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.a Categories Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, parents, values, ids = create_treemap_data(df_categories)\n",
    "fig = go.Figure(go.Treemap(\n",
    "    labels=labels,\n",
    "    parents=parents,\n",
    "    values=values,\n",
    "    ids=ids,\n",
    "    textfont=dict(size=18),\n",
    "))\n",
    "fig.update_layout(margin=dict(t=50, l=10, r=10, b=5), title=\"Category Distribution in Articles Counting Every Category for Each Article\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have {df_categories[\"level_1\"].unique().size} distinct level 1 categories.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each category is organized into multiple sub-levels, with a hierarchical depth of up to 3 levels.\n",
    "\n",
    "- For this analysis, we will focus exclusively on the most superficial level: Level 1.\n",
    "\n",
    "- Additionally, we observe that some articles are associated with multiple categories, highlighting overlaps and shared connections within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Articles with Multiples Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we count the numbers of articles with multiples categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of articles with multiples categories\n",
    "df_categories.groupby(\"article\")[\"article\"].size().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But if we constrain it only to different type of level 1 categories, it is reduced to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique level 1 categories per article\n",
    "df_categories.groupby(\"article\")[\"level_1\"].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_abbreviations = {\n",
    "    'Art': 'Art',\n",
    "    'Mathematics': 'Math',\n",
    "    'IT': 'IT',\n",
    "    'Business Studies': 'BS',\n",
    "    'Music': 'Music',\n",
    "    'Religion': 'R',\n",
    "    'Language and literature': 'L&L',\n",
    "    'Citizenship': 'CIT',\n",
    "    'Countries': 'C',\n",
    "    'Design and Technology': 'D&T',\n",
    "    'Everyday life': 'Life',\n",
    "    'History': 'Hist',\n",
    "    'People': 'P',\n",
    "    'Geography': 'Geo',\n",
    "    'Science': 'Sci'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cooccurrence_cat_matrix(df_categories, category_abbreviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributing the **main category** to articles with multiple categories based on the category **with fewer total articles** helps prioritize specialization over generality. \n",
    "\n",
    "Categories with fewer total articles are typically more specific, while those with higher counts cover broader topics. By focusing on the category with fewer articles, we ensure the articleâ€™s primary focus is on a unique or specialized perspective, providing a clearer thematic assignment. This method promotes a balanced classification system, ensuring articles are categorized accurately without being overshadowed by more general categories.\n",
    "\n",
    "- For example, a category like **\"Geography\"** may encompass a wide range of topics, while **\"Countries\"** might be more specialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories_filtered = filter_most_specific_category(df_categories)\n",
    "\n",
    "labels_filtered, parents_filtered, values_filtered, ids_filtered = create_treemap_data(df_categories_filtered)\n",
    "fig = go.Figure(go.Treemap(\n",
    "    labels=labels_filtered,\n",
    "    parents=parents_filtered,\n",
    "    values=values_filtered,\n",
    "    ids=ids_filtered,\n",
    "    textfont=dict(size=18),\n",
    "))\n",
    "\n",
    "fig.update_layout(margin=dict(t=50, l=10, r=10, b=5), title=\"Category Distribution in Articles (Only the most specific category is shown for each article)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.b Transitions between categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to merge the paths finished and unfinished as in this section we will first only analyse how users make moves between categories of articles, independ of their succes on the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_merged = pd.concat([df_finished, df_unfinished])\n",
    "common_paths = analyze_categories_paths(paths_merged, df_categories_filtered, omit_loops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_f = matrix_common_paths(common_paths)\n",
    "transition_cat_matrix(matrix_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_articles_pie_chart(df_categories_filtered, category_abbreviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transitions within categories (including self-category)\n",
    "\n",
    "Example :\n",
    "- Science -> Science -> Science\n",
    "- Science -> Science -> Science -> Citizenship -> Design and Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_position_data = get_position_frequencies(common_paths, max_position=15)\n",
    "plot_position_line(df_position_data, title=\"Position Frequencies for Finished Paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transitions within categories (excluding self-category)\n",
    "\n",
    "Example :\n",
    "- Science -> Science -> Science **becomes** Science\n",
    "- Science -> Science -> Science -> Citizenship -> Design and Technology **becomes** Science -> Citizenship -> Design and Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_paths_nl = analyze_categories_paths(paths_merged, df_categories_filtered, omit_loops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_position_data = get_position_frequencies(common_paths_nl, max_position=15)\n",
    "plot_position_line(df_position_data, title=\"Position Frequencies for Finished Paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transitions within categories of the optimal paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the frequency of chosen categories per position by users with the categories attributed to the optimal path, in order to see if it is effectively a good strategy to pass by 'country' or 'geography' at the beginning to reach the target. In order to do that we first need to find the optimal path, and thus build a directed graph with edges corresponding to connection between articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_paths = find_all_source_target_pairs(df_finished, df_unfinished, df_links)\n",
    "optimal_paths = calculate_optimal_path(df_links, optimal_paths, df_shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_paths = analyze_categories_paths(optimal_paths, df_categories_filtered, users=False, omit_loops=False)\n",
    "df_position_opt_data = get_position_frequencies(opt_paths, max_position=15)\n",
    "plot_position_line(df_position_opt_data, title=\"Position Frequencies for Optimal Paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_paths_nl = analyze_categories_paths(optimal_paths, df_categories_filtered, users=False, omit_loops=True)\n",
    "df_position_opt_data = get_position_frequencies(opt_paths_nl, max_position=15)\n",
    "plot_position_line(df_position_opt_data, title=\"Position Frequencies for Optimal Paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maybe 3.3.c Start And Target Categories in finished and unfinished paths\n",
    "\n",
    "ðŸš¨ ici ce serait pas mieux de mettre les counts en pourcentage? ðŸš¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_finished = find_categories_start_end(df_finished, df_categories)\n",
    "paths_unfinished = find_categories_start_end(df_unfinished, df_categories)\n",
    "\n",
    "paths_finished[\"finished\"] = 1\n",
    "paths_unfinished[\"finished\"] = 0\n",
    "\n",
    "paths = pd.concat([paths_finished, paths_unfinished], join='inner')\n",
    "paths_melted = paths.melt(\n",
    "    id_vars=[\"finished\"],\n",
    "    value_vars=[\"start_category\", \"end_category\"],\n",
    "    var_name=\"category_type\",\n",
    "    value_name=\"category\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "\n",
    "# Create a color palette based on unique categories\n",
    "unique_categories = paths_melted['category'].unique()\n",
    "palette = sn.color_palette(\"Set1\", n_colors=len(unique_categories))\n",
    "color_mapping = dict(zip(unique_categories, palette))\n",
    "\n",
    "g = sn.FacetGrid(\n",
    "    data=paths_melted,\n",
    "    col=\"finished\",\n",
    "    row=\"category_type\",\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    "    margin_titles=True,\n",
    "    height=4,\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sn.countplot,\n",
    "    x=\"category\",\n",
    "    hue=\"category\",\n",
    "    palette=color_mapping,\n",
    "    order=paths_melted['category'].value_counts().index  # Order by frequency\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "for ax in g.axes.flatten():\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.c Networks\n",
    "\n",
    "EXPLAIN NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_merged_with_replace_back = paths_merged['path'].apply(replace_back_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_network(paths_merged_with_replace_back, df_categories_filtered, include_self_loops=False)\n",
    "\n",
    "plot_network(G, title=\"Networks of transition whithin Categoires (exclude self loops)\", show_edge_labels=False, node_size=1000, node_abbreviations=category_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_normalized = normalize_edge_weights(G, df_categories_filtered)\n",
    "plot_network(G_normalized, title=\"Networks normalize by the category size of the source node (exclude self loops)\", show_edge_labels=False, node_size=1000, node_abbreviations=category_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_edge_weights(G_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Correlations and combination between previous results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.a Corelation between difficulty and combination of source-target categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a glance at the extend to which the choice of the source article and the end article affects the difficulty of the game.\n",
    "This could highlight potential combinations of categories that are harder to connect.\n",
    "\n",
    "Different difficulty metrics are used like path duration, path length in terms of number of clicks and user ratings. \n",
    "\n",
    "The number of backclicks and its link with current difficulty mettrics and categories is also studied to explore whether it indicates difficulty as well.\n",
    "#### Correlation between the duration of game and the combination of source-target category\n",
    "Here the difficulty measure is the game duration. \n",
    "\n",
    "ðŸš¨put different colors for finished ans unfinished, so the colorbar isnt the same for diffrent scalesðŸš¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "path_finished_length_categories = paths_finished.pivot_table(index='start_category', \n",
    "                                                    columns='end_category', \n",
    "                                                    values='durationInSec', \n",
    "                                                    aggfunc='mean')\n",
    "path_unfinished_length_categories = paths_unfinished.pivot_table(index='start_category',\n",
    "                                                    columns='end_category',\n",
    "                                                    values='durationInSec',\n",
    "                                                    aggfunc='mean')\n",
    "\n",
    "sn.heatmap(path_finished_length_categories , cmap='BuPu', ax=ax[0])\n",
    "sn.heatmap(path_unfinished_length_categories , cmap='BuPu', ax=ax[1])\n",
    "ax[0].set_title(\"Finished Paths\")\n",
    "ax[1].set_title(\"Unfinished Paths\")\n",
    "fig.suptitle(\"Heatmap of Duration (seconds) for Each Start-End Category Combination\")\n",
    "plt.xlabel(\"End Category\")\n",
    "plt.ylabel(\"Start Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linking a music article to a language and litterature article is the combination that takes the longest time (â‰ˆ800s) to connect in finished paths. In unfinished paths, it takes â‰ˆ1300s, maybe indicating players tend to abandon due to difficulty? \n",
    "\n",
    "Curiously, connnecting art to mathematics is either very fast (<100s) in succesfull games, or takes a very long time (â‰ˆ2000s) in unsuccesfull ones, this is also observed in connecting mathematics to mathematics for example.\n",
    "\n",
    "In the other hand, connecting religion to religion takes a short time both in succesfull and unsuccesfull games. A short finished path might indicate easily connectable categories, whereas short unfinished paths could indicate early abandonment. Why would easilly conectable source and target lead to early abandonment for other players? Maybe this scenario can divide into two : either the target is directly on the source page or just a few clicks away, either they are not so close (for example linked to different religions), the density of links in the page might be low and the player might lack knowledge in the field. These potential causes will be analize in further parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between the length of path and the combination of source-target category\n",
    "Here the difficulty metric used is the number of clicks in the game. \n",
    "\n",
    "ðŸš¨put different colors for finished ans unfinished, so the colorbar isnt the same for diffrent scalesðŸš¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_finished[\"steps_count\"] = paths_finished[\"path\"].apply(lambda x: len(x.split(\";\")))\n",
    "paths_unfinished[\"steps_count\"] = paths_unfinished[\"path\"].apply(lambda x: len(x.split(\";\")))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "path_finished_length_categories = paths_finished.pivot_table(index='start_category', \n",
    "                                                    columns='end_category', \n",
    "                                                    values='steps_count', \n",
    "                                                    aggfunc='mean')\n",
    "path_unfinished_length_categories = paths_unfinished.pivot_table(index='start_category',\n",
    "                                                    columns='end_category',\n",
    "                                                    values='steps_count',\n",
    "                                                    aggfunc='mean')\n",
    "\n",
    "sn.heatmap(path_finished_length_categories , cmap='BuPu', ax=ax[0])\n",
    "sn.heatmap(path_unfinished_length_categories , cmap='BuPu', ax=ax[1])\n",
    "ax[0].set_title(\"Finished Paths\")\n",
    "ax[1].set_title(\"Unfinished Paths\")\n",
    "fig.suptitle(\"Heatmap of length of path (number of steps) for Each Start-End Category Combination\")\n",
    "plt.xlabel(\"End Category\")\n",
    "plt.ylabel(\"Start Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between the player rating and the combination of source-target category\n",
    "Here we use the difficulty rating by the players as difficulty metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_finished_length_categories = paths_finished.pivot_table(index='start_category', \n",
    "                                                    columns='end_category', \n",
    "                                                    values='rating', \n",
    "                                                    aggfunc='mean')\n",
    "\n",
    "sn.heatmap(path_finished_length_categories , cmap='BuPu')\n",
    "plt.title(\"Heatmap of rating for Each Start-End Category Combination\")\n",
    "plt.xlabel(\"End Category\")\n",
    "plt.ylabel(\"Start Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Methods for Further Analysis\n",
    "This section aims to show a few of the tools that will be useful along the way for our project. Instead of using them to answer our final question, we use them on the dataset as a whole to show that interesting information can be extracted from these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Similarity on paths\n",
    "\n",
    "Let's first get an intuition of how shortest paths are distributed and how articles are generally connected. The shortest path ranges from 1 to 9 if it exists, but it may also not exist at all (value -1). This means there are clusters of articles that are not linked to each other in the network. We can visualise this with a matrix that shows whether certain source and target articles are connected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shortest_paths_matrix(df_shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df_shortest_path.values.flatten(), bins=np.arange(-1,11,1) - 0.5, alpha=0.6, edgecolor='black', color='black')\n",
    "plt.xlabel('Shortest Path Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Shortest Path Distances')\n",
    "plt.xticks(np.arange(-1,10,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.a Semantic Similarity\n",
    "\n",
    "An interesting way to figure out how players move through the wikispeedia network is semantic similarity. This encompasses both categories similarity and the abstract notion of \"meaning\". To concretise this notion, we consider two different measures of similarity:\n",
    "- semantic similarity based on article names embeddings\n",
    "- semantic similarity based on the Jaccard similarity of categories (with stronger weights for more specific categories)\n",
    "\n",
    "An interesting application is finding clusters of words that are more similar to each other than to words in other clusters. The clustering allows to reorder the similarity matrix in a way that regroups articles with high (or low) similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering\n",
    "sm = df_sm.to_numpy()\n",
    "linkage_matrix = linkage(sm, method='ward')\n",
    "ordered_indices = leaves_list(linkage_matrix)\n",
    "reordered_matrix = sm[ordered_indices, :][:, ordered_indices]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(reordered_matrix, cmap='BuPu')\n",
    "plt.title('Reordered Similarity Matrix')\n",
    "plt.xlabel('Reordered Article Index')\n",
    "plt.ylabel('Reordered Article Index')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial threshold\n",
    "initial_threshold = 0.5\n",
    "\n",
    "# Function to plot the binary matrix based on a given threshold\n",
    "def plot_thresholded_matrix(threshold):\n",
    "    # Create a binary mask with the current threshold\n",
    "    binary_matrix = np.where(reordered_matrix >= threshold, 1, 0)\n",
    "    \n",
    "    # Plot the binary matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(binary_matrix, cmap='Reds', interpolation='nearest')\n",
    "    plt.title(f'Reordered Similarity Matrix (Threshold = {threshold:.2f})')\n",
    "    plt.xlabel('Reordered Article Index')\n",
    "    plt.ylabel('Reordered Article Index')\n",
    "    plt.colorbar(label='Above Threshold (1 = Red, 0 = White)')\n",
    "    plt.show()\n",
    "\n",
    "# Create the interactive slider\n",
    "threshold_slider = widgets.FloatSlider(\n",
    "    value=initial_threshold,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Threshold:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# Display the slider and link it to the plotting function\n",
    "output = widgets.interactive_output(plot_thresholded_matrix, {'threshold': threshold_slider})\n",
    "display(threshold_slider, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./figures/41a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the reordered article indices make intuitive sense, similar words are regrouped. Words in the clusters with high similarity (similarity > 0.5) are words that appear more often in natural language, while words in the low-similarity areas are similar one to another, but not commonly used. This shows that we can regroup articles in clusters of 'well-known words' that are likely more present on user paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Words in high similarity clusters are for example {\", \".join(df_article_names[ordered_indices].iloc[1995:2005].tolist())}')\n",
    "print(f'Words in high similarity clusters are for example {\", \".join(df_article_names[ordered_indices].iloc[-7:].tolist())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.b Correlation between source-target similarity, and shortest possible path length\n",
    "Similar articles should be easier to connect, let's verify this using shortest possible path length as a difficulty measure.\n",
    "\n",
    "We will look at article's semantic symilarity using cosine similarity, and article's category similarity. Pearson and Spearman correlations will then be used to asses how significant these difficulty measures are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reds_palette = sn.dark_palette(\"salmon\", n_colors=len(df_finished['shortest_path'].unique()), reverse=True)\n",
    "blues_palette = sn.dark_palette(\"#69d\", n_colors=len(df_unfinished['shortest_path'].unique()), reverse=True)\n",
    "blues_palette = [blues_palette[-1]] + blues_palette[:-1] # put impossible paths in evidence\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Finished paths plot\n",
    "ax = plt.subplot(121)\n",
    "sn.barplot(data=df_finished, x='shortest_path', y='cosine_similarity', hue='shortest_path', legend=None, palette=reds_palette)\n",
    "plt.title('Finished paths')\n",
    "\n",
    "# Unfinished paths plot\n",
    "plt.subplot(122, sharey=ax)\n",
    "sn.barplot(data=df_unfinished, x='shortest_path', y='cosine_similarity', hue='shortest_path', legend=None, palette=blues_palette)\n",
    "plt.title('Unfinished paths')\n",
    "\n",
    "plt.suptitle('Mean cosine similarity per shortest path length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def calculate_correlations_with_shortestPath(df, column_name):\n",
    "    # Ensure 'shortest_path' is numeric\n",
    "    df['shortest_path'] = df['shortest_path'].astype(float)\n",
    "\n",
    "    pearson_corr, pearson_p = pearsonr(df['shortest_path'], df[column_name])\n",
    "    spearman_corr, spearman_p = spearmanr(df['shortest_path'], df[column_name])\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Pearson correlation: {pearson_corr:.4f}, p-value: {pearson_p:.4e}\")\n",
    "    print(f\"Spearman correlation: {spearman_corr:.4f}, p-value: {spearman_p:.4e}\")\n",
    "\n",
    "print('Finished paths:')\n",
    "finished_results = calculate_correlations_with_shortestPath(df_finished,\"cosine_similarity\")\n",
    "print('\\nUnfinished paths:')\n",
    "unfinished_results = calculate_correlations_with_shortestPath(df_unfinished,\"cosine_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "ax = plt.subplot(121)\n",
    "sn.barplot(data=df_finished, x='shortest_path', y='categories_similarity', hue='shortest_path', legend=None, palette=reds_palette)\n",
    "plt.title('Finished paths')\n",
    "\n",
    "plt.subplot(122, sharey=ax)\n",
    "sn.barplot(data=df_unfinished, x='shortest_path', y='categories_similarity', hue='shortest_path', legend=None, palette=blues_palette)\n",
    "plt.title('Unfinished paths')\n",
    "\n",
    "plt.suptitle('Mean Categories Similarity per Shortest Path Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finished paths:')\n",
    "finished_results = calculate_correlations_with_shortestPath(df_finished,\"categories_similarity\")\n",
    "print('\\nUnfinished paths:')\n",
    "unfinished_results = calculate_correlations_with_shortestPath(df_unfinished,\"categories_similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pearson correlation** measures the linear relationship between two continuous variables. And the **Spearman correlation** assesses the monotonic relationship using ranked data, making it less sensitive to outliers and non-linear relationships.\n",
    "\n",
    "\n",
    "The article semantic similarity, and category similarity of finished paths have **moderate negative correlation** with path length (â‰ˆ-0.25), and the category similarity of unfinished paths has weak negative correlation (â‰ˆ-0.2).\n",
    "\n",
    "Both for articles and category similarity, the **p-values are zero**, showing high statistical significance. The observed correlations are thus unlikely to be due to random chance.\n",
    "\n",
    "Negative correlation indicates that as the minimal possible path length increases, the cosine similarity decreases.\n",
    "In other words, when the source and targets articles are close semantically, and their categories similar, the shortest possible path shrinkens, which makes sense as is it easier to connect 2 very similar articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Evolution of similarity on paths\n",
    "Now that we established similarity as a metric, can we see how mean similarity evolves on paths in general? Do we observe that users quickly try to leave a category in the first few clicks? Later this will be used to assess whether there is a difference for voyages and non-voyages, and whether there is a difference depending on the categories the path in made of, but for now this already gives an insight into what we can expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_finished_paths = [replace_back_clicks(path).split(';') for path in df_finished['path'].tolist()]\n",
    "path_similarities = []\n",
    "\n",
    "for path in all_finished_paths:\n",
    "    path_similarity = []\n",
    "    for step in range(len(path)-1):\n",
    "        current, next = path[step], path[step+1]\n",
    "        path_similarity.append(df_sm[current][next])\n",
    "\n",
    "    path_similarities.append(path_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example to understand what is going on. Each point represents a click of a user, to go from the first page (Batman) to the next (Chemistry). Batman and Chemistry are not strongly semantically related (hence the relatively low value of about 0.4). In the next step to Biology, the similarity is much higher (about 0.7), so the player first leave the category to reach the target category and then tends to stay there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_id = 1829\n",
    "path = path_similarities[path_id]\n",
    "plt.plot(range(len(path)), path, marker='o', color='#0c8714')\n",
    "plt.xticks([i-0.5 for i in range(len(path)+1)], all_finished_paths[path_id], rotation=90)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_N_path_similarity = {}\n",
    "for path_sim in path_similarities:\n",
    "    path_length = len(path_sim)\n",
    "    len_N_path_similarity.setdefault(path_length, []).append(path_sim)\n",
    "\n",
    "len_N_mean_similarity = {paths_len: np.mean(paths, axis=0) for paths_len, paths in len_N_path_similarity.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "greens_palette = sn.light_palette(\"#0c8714\", n_colors=10)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    sn.lineplot(len_N_mean_similarity[i+1], lw=1, label=i, color=greens_palette[i-1])\n",
    "plt.legend(title='# Link Clicks')\n",
    "plt.xlabel(\"Position of the Click in the Players' Paths\")\n",
    "plt.xticks(range(0, 11))\n",
    "plt.ylabel('Mean Semantic Similarity')\n",
    "plt.title('Mean Semantic Similarity Difference Aggregated by Path Lenghts')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear trend: in the first few clicks, the articles chosen have a low similarity to the previous one: this can be interpreted as leaving the original category. After this, the similarity for the next clicks stabilises, with only small fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Find Voyages\n",
    "### Sort games into voyage or not-voyage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finished_voyage = game_voyage_sorting(df_finished, df_categories, True, n=3)\n",
    "df_unfinished_voyage = game_voyage_sorting(df_unfinished, df_categories, False, n=3)\n",
    "\n",
    "voyage_count = (df_finished_voyage['voyage'] == 1).sum() + (df_unfinished_voyage['voyage'] == 1).sum()\n",
    "non_voyage_count = (df_finished_voyage['voyage'] == 0).sum() + (df_unfinished_voyage['voyage'] == 0).sum()\n",
    "print('out of', len(df_finished_voyage)+len(df_unfinished_voyage), 'games : ')\n",
    "print('  - ', voyage_count, ' are voyages')\n",
    "print('  - ', non_voyage_count, ' are not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for start, voyage, and end nodes\n",
    "df_finished_voyage['start_category_label'] = df_finished_voyage['start_category'].apply(lambda x: 'First in Countries/Geography' if x in ['Countries', 'Geography'] else 'First not in Countries/Geography')\n",
    "df_finished_voyage['end_category_label'] = df_finished_voyage['end_category'].apply(lambda x: 'Target in Countries/Geography' if x in ['Countries', 'Geography'] else 'Target not in Countries/Geography')\n",
    "df_finished_voyage['voyage_label'] = df_finished_voyage['voyage'].apply(lambda x: 'Voyage' if x else 'Non-Voyage')\n",
    "\n",
    "# Count occurrences for each flow from start -> voyage -> end\n",
    "flows = df_finished_voyage.groupby(['start_category_label', 'voyage_label', 'end_category_label']).size().reset_index(name='count')\n",
    "\n",
    "# Define nodes for the Sankey diagram\n",
    "labels = ['First in Countries/Geography', 'First not in Countries/Geography',\n",
    "          'Voyage', 'Non-Voyage',\n",
    "          'Target in Countries/Geography', 'Target not in Countries/Geography']\n",
    "\n",
    "# Create mappings for source and target node indices\n",
    "label_map = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Map flows to Sankey source, target, and value arrays\n",
    "sources = []\n",
    "targets = []\n",
    "values = []\n",
    "\n",
    "for _, row in flows.iterrows():\n",
    "    # Add start -> voyage\n",
    "    sources.append(label_map[row['start_category_label']])\n",
    "    targets.append(label_map[row['voyage_label']])\n",
    "    values.append(row['count'])\n",
    "\n",
    "    # Add voyage -> end\n",
    "    sources.append(label_map[row['voyage_label']])\n",
    "    targets.append(label_map[row['end_category_label']])\n",
    "    values.append(row['count'])\n",
    "\n",
    "\n",
    "node_colors = [\n",
    "    \"rgba(0, 128, 128, 0.6)\",  \"rgba(244,109,67,0.8)\",   \n",
    "    \"rgba(0, 128, 128, 0.6)\",  \"rgba(244,109,67,0.8)\",   \n",
    "    \"rgba(0, 128, 128, 0.6)\",  \"rgba(244,109,67,0.8)\"\n",
    "    ]\n",
    "\n",
    "link_colors = [\n",
    "    \"rgba(146,197,222,0.6)\",  \"rgba(244,109,67,0.6)\",   \n",
    "    \"rgba(146,197,222,0.6)\",  \"rgba(244,109,67,0.6)\",   \n",
    "    \"rgba(146,197,222,0.6)\",  \"rgba(244,109,67,0.6)\"\n",
    "    ]\n",
    "\n",
    "# Create the Sankey plot\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(pad=15, thickness=20, line=dict(color=\"white\", width=0.1), label=labels, color=node_colors),\n",
    "    link=dict(source=sources, target=targets, value=values)\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Path classification as Voyage or not\",\n",
    "    font_size=10,\n",
    "    title_font_size=14,\n",
    "    title_x=0.5,\n",
    "    plot_bgcolor=\"white\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analyse voyages\n",
    "### âžœ This is our milestone P3!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_path_matrix = read_shortest_path_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of article pairs\n",
    "total_pairs = shortest_path_matrix.size\n",
    "\n",
    "# Number of reachable pairs (distance from 1 to 9)\n",
    "# Exclude self-pairs where distance is 0\n",
    "# Unreachable pairs are represented by -1\n",
    "\n",
    "# Create a mask for self-pairs (distance == 0)\n",
    "self_pairs_mask = (shortest_path_matrix == 0)\n",
    "\n",
    "# Create a mask for reachable pairs (distance between 1 and 9)\n",
    "reachable_mask = (shortest_path_matrix >= 1) & (shortest_path_matrix <= 9)\n",
    "\n",
    "\n",
    "reachable_pairs = np.count_nonzero(reachable_mask)\n",
    "unreachable_pairs = np.count_nonzero(shortest_path_matrix == -1)\n",
    "\n",
    "# Sparsity percentage: proportion of unreachable pairs\n",
    "sparsity_percentage = ((unreachable_pairs + len(self_pairs_mask) ) / total_pairs) * 100\n",
    "\n",
    "print(f\"Total pairs: {total_pairs}\")\n",
    "print(f\"Reachable pairs: {reachable_pairs}\")\n",
    "print(f\"Unreachable pairs: {unreachable_pairs}\")\n",
    "print(f\"Sparsity percentage: {sparsity_percentage:.2f}%\")\n",
    "\n",
    "# Total number of article pairs\n",
    "total_pairs = shortest_path_matrix.size\n",
    "\n",
    "# Number of reachable pairs (distance from 1 to 9)\n",
    "# Exclude self-pairs where distance is 0\n",
    "# Unreachable pairs are represented by -1\n",
    "\n",
    "# Create a mask for self-pairs (distance == 0)\n",
    "self_pairs_mask = (shortest_path_matrix == 0)\n",
    "\n",
    "# Create a mask for reachable pairs (distance between 1 and 9)\n",
    "reachable_mask = (shortest_path_matrix >= 1) & (shortest_path_matrix <= 9)\n",
    "\n",
    "\n",
    "reachable_pairs = np.count_nonzero(reachable_mask)\n",
    "unreachable_pairs = np.count_nonzero(shortest_path_matrix == -1)\n",
    "\n",
    "# Sparsity percentage: proportion of unreachable pairs\n",
    "sparsity_percentage = ((unreachable_pairs + len(self_pairs_mask) ) / total_pairs) * 100\n",
    "\n",
    "print(f\"Total pairs: {total_pairs}\")\n",
    "print(f\"Reachable pairs: {reachable_pairs}\")\n",
    "print(f\"Unreachable pairs: {unreachable_pairs}\")\n",
    "print(f\"Sparsity percentage: {sparsity_percentage:.2f}%\")\n",
    "\n",
    "# Create a binary matrix where 1 represents a reachable path and 0 represents an unreachable path\n",
    "sparsity_matrix = np.where(shortest_path_matrix == -1, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(sparsity_matrix, cmap='Greys', interpolation='nearest')\n",
    "plt.title('Sparsity Pattern in Shortest Path Matrix')\n",
    "plt.xlabel('Target Article')\n",
    "plt.ylabel('Source Article')\n",
    "plt.colorbar(label='Reachability (1=Reachable, 0=Unreachable)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(shortest_path_matrix.values.flatten(), bins=np.arange(-1,11,1) - 0.5 , color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Shortest Path Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Shortest Path Distances')\n",
    "plt.xticks(np.arange(-1,10,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
